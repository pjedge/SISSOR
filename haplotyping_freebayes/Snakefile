# author: Peter Edge
# 3/29/2016
# email: pedge@eng.ucsd.edu

# this is a snakemake Snakefile, written using snakemake 3.5.5
# configure paths and values in cluster.yaml and config.yaml to your system
# example execution for a TORQUE cluster:
# snakemake -j 200 --cluster-config cluster.yaml --cluster "qsub -A {cluster.group} -V -q {cluster.queue} -o {cluster.qsub_stdout_dir}/{params.job_name}.o -e {cluster.qsub_stdout_dir}/{params.job_name}.e -N {params.job_name} -l nodes=1:ppn={cluster.ppn} -l walltime={cluster.walltime} -M {cluster.email} -m e -d {cluster.working_dir}" --local-cores 1
sys.path.append('/home/peter/git/HapTools')
sys.path.append('/home/pedge/git/hapcut2/utilities')
configfile: "config.yaml"
localrules: all, plot_hapcut2_results, simlinks, clean

import sys
sys.path.append(config['haptools_dir'])
import run_tools
import calculate_haplotype_statistics as chs
import fileIO
import plot_data
import os
from os.path import join
from create_hapcut_fragment_matrix import create_hapcut_fragment_matrices_freebayes
from fix_chamber_contamination import fix_chamber_contamination
import pickle
from collections import defaultdict
from plot_sissor import plot_sissor
from copy import copy
from freebayes_strand_pair import freebayes_strand_pair, accuracy_aggregate, split_vcf
from itertools import product
import strand_pairing_analysis

# qsub stderr and stdout directories are not automatically created by snakemake!
if not os.path.exists(config['qsub_stdout_dir']):
    os.makedirs(config['qsub_stdout_dir'])

chunksize = 25000000

hg19_size_list = [('chr1', 249250621),
 ('chr2', 243199373),
 ('chr3', 198022430),
 ('chr4', 191154276),
 ('chr5', 180915260),
 ('chr6', 171115067),
 ('chr7', 159138663),
 ('chr8', 146364022),
 ('chr9', 141213431),
 ('chr10', 135534747),
 ('chr11', 135006516),
 ('chr12', 133851895),
 ('chr13', 115169878),
 ('chr14', 107349540),
 ('chr15', 102531392),
 ('chr16', 90354753),
 ('chr17', 81195210),
 ('chr18', 78077248),
 ('chr19', 59128983),
 ('chr20', 63025520),
 ('chr21', 48129895),
 ('chr22', 51304566),
 ('chrX', 155270560),
 ('chrY', 59373566)]


# create chunks of hg19
# return list of (chrom,start,stop) tuples. stop is inclusive
chunklist = []

for chrom, chrlen in hg19_size_list:
    for start in range(1,chrlen+1,chunksize):
        end = start+chunksize-1 if start+chunksize-1 < chrlen else chrlen
        chunklist.append((chrom,start,end))

regions = ['{}.{}.{}'.format(chrom,start,stop) for chrom,start,stop in chunklist]


# specify chambers
data_dir = config["data_dir"]
experiments_dir = config["experiments_dir"]
plots_dir = config["plots_dir"]
chambers = list(range(1,25))
chambers_pad = ['{0:02d}'.format(c) for c in chambers]
chroms  = ['chr{}'.format(i) for i in range(1,23)]
chroms_XY  = ['chr{}'.format(i) for i in range(1,23)]
samples = ['PGP1_ALL','PGP1_21','PGP1_22','PGP1_A1']
cells=samples[1:]

variant_vcf_dir = 'PGP1_VCFs'
variant_vcf_dir_fp = join(data_dir,'PGP1_VCFs_w_false_positives')

rule all:
    input:
         #'sissor_project/experiments/strand_pair/within_cell.counts.p',
         #'sissor_project/experiments/strand_pair/between_cell.counts.p'
        #expand("{P}/sissor_haplotype_error_chromosome.png",P=config['plots_dir']),
        expand("{P}/sissor_haplotype_error_genome.png",P=config['plots_dir']),
        #expand('{d}/{cell}/ploidy1_split/ch{ch}.{chrom}.vcf',d=data_dir,cell=cells,ch=chambers,chrom=chroms)
        #'strand_pairs/all'


# PLOT RESULTS

# PLOT RESULTS
HAPLOTYPE_ERROR_RATES_DIR = 'sissor_project/error_rates'
HAPLOTYPE_PLOTS_DIR = 'sissor_project/plots'

rule plot_hapcut2_results:
    params:
        job_name = "plot_hapcut2_sissor"
    input:
        stats_file  = "{}/hapcut2.stats.p".format(HAPLOTYPE_ERROR_RATES_DIR),
        labels_file = "{}/hapcut2.labels.p".format(HAPLOTYPE_ERROR_RATES_DIR)
    output:
        plot2 = "%s/sissor_haplotype_error_genome.png" % HAPLOTYPE_PLOTS_DIR
    run:
        data = pickle.load(open(input.stats_file,"rb"))
        labels = pickle.load(open(input.labels_file,"rb"))
        plot_sissor(data,labels,output.plot2)

exp =['cov1_none','cov1_basic','cov1_strict','cov2_basic']
exp_labels=['No processing','Basic Processing','Strict Processing','Basic Processing, coverage >= 2']
rule calculate_error_rates:
    params:
        job_name = "hapcut2_error_rates"
    input:
        hapblocks = expand("{E}/hapcut2_PGP1_ALL/{exp}/{c}.output",E=experiments_dir,exp=exp,c=chroms),
        runtimes  = expand("{E}/hapcut2_PGP1_ALL/{exp}/{c}.runtime",E=experiments_dir,exp=exp,c=chroms),
        fragmats  = expand("{d}/PGP1_ALL/fragmat/{exp}/{c}",d=data_dir,exp=exp,c=chroms),
        var_vcfs  = expand("{v}/{c}.vcf",v=variant_vcf_dir,c=chroms),
        bac_haps  = expand("{bac}/{c}.filtered",bac=config['BAC_hapblocks'], c=chroms),
        contigsizefile = '/home/pedge/git/hapcut2/utilities/hg19.chrom.sizes'
    output:
        stats_file  = "{}/hapcut2.stats.p".format(config['error_rates_dir']),
        labels_file = "{}/hapcut2.labels.p".format(config['error_rates_dir'])
    run:
        # list of lists of error results,
        data   = [] # index of the list is a 'condition' each inner list has 23 error results (each chrom).
        labels = exp_labels

        for x in exp: # sample
            datalist = []
            for c in chroms: # chromosome
                assembly_file = "{}/hapcut2_PGP1_ALL/{}/{}.output".format(config['experiments_dir'],x,c)
                runtime_file  = "{}/hapcut2_PGP1_ALL/{}/{}.runtime".format(config['experiments_dir'],x,c)
                frag_file     = "{}/PGP1_ALL/fragmat/{}/{}".format(data_dir,x,c)
                vcf_file      = "{}/{}.vcf".format(variant_vcf_dir,c)
                truth_file    = "{}/{}.filtered".format(config['BAC_hapblocks'], c)
                err = chs.hapblock_hapblock_error_rate(truth_file, vcf_file, assembly_file, frag_file, vcf_file, input.contigsizefile)
                datalist.append(err)

            print("{} results over all chromosomes:".format(x))
            print(sum(datalist,chs.error_result()))

            data.append(datalist)

        pickle.dump(data,open(output.stats_file,"wb"))
        pickle.dump(labels,open(output.labels_file,"wb"))
'''

rule accuracy_aggregate:
    params: job_name = 'accuracy_aggregate.{report_name,(within_cell|between_cell)}'
    input:  counts = expand('sissor_project/experiments/strand_pair/{{report_name,(within_cell|between_cell)}}/split/{r}.counts.p',r=regions),
            mof = expand('sissor_project/experiments/strand_pair/{{report_name,(within_cell|between_cell)}}/split/{r}.mismatches.txt',r=regions)
    output: counts = 'sissor_project/experiments/strand_pair/{report_name,(within_cell|between_cell)}.counts.p',
            mof = 'sissor_project/experiments/strand_pair/{report_name,(within_cell|between_cell)}.mismatches'
    run:
        accuracy_aggregate(input.counts,output.counts)
        shell('cat {input.mof} > {output.mof}')

rule freebayes_strand_pairing:
    params: job_name = 'freebayes_strand_pairing.{r,chr([0-9]+|X|Y)\.[0-9]+\.[0-9]+}'
    input: strand_pairs = 'strand_pairs/all',
           vcfs = expand('sissor_project/data/{P[0]}/ploidy1_split/ch{P[1]}.{{r,chr([0-9]+|X|Y)\.[0-9]+\.[0-9]+}}.vcf',P=product(cells,chambers)),
           gff = '/oasis/tscc/scratch/wkchu/SISSOR/PGP1_A1/BAM/ns.gff',
           fasta = '/oasis/tscc/scratch/pedge/data/genomes/hg19/hg19.fa',
    output: counts = 'sissor_project/experiments/strand_pair/split/{r,chr([0-9]+|X|Y)\.[0-9]+\.[0-9]+}.counts.p',
            mismatch_file = 'sissor_project/experiments/strand_pair/split/{r,chr([0-9]+|X|Y)\.[0-9]+\.[0-9]+}.mismatches.txt'
    run:
        freebayes_strand_pair(input.vcfs, input.strand_pairs, input.gff, input.fasta, output.counts, output.mismatch_file, same_cell_only)

rule split_vcfs:
    params: job_name = 'split_vcf.{cell}.{ch}'
    input:  vcf = '{data_dir}/{cell}/ploidy1/ch{ch}.vcf'
    output: split = expand('{{data_dir}}/{{cell}}/ploidy1_split/ch{{ch}}.{r}.vcf',r=regions)
    run:
        split_vcf(input.vcf, chunklist, output.split)


rule combine_paired_strand_files:
    params: job_name = 'combine_paired_strand_files'
    input:  sep = expand('strand_pairs/{chrom}',chrom=chroms+['chrXchrY']),
    output: combined = 'strand_pairs/all',
    shell:  'cat {input.sep} > {output.combined}'

rule pair_strands:
    params: job_name = 'pair_strands'
    input: frag = 'sissor_project/data/PGP1_ALL/fragmat/cov1_strict/{chrom}',
           vcf  = 'PGP1_VCFs/{chrom}.vcf',
           hap  = 'sissor_project/experiments/hapcut2_PGP1_ALL/cov1_strict/{chrom}.output'
    output: sep = 'strand_pairs/{chrom}',
    run:
        strand_pairing_analysis.pair_strands(input.frag,input.vcf,output.sep,input.hap)

rule pair_strands_XY:
    params: job_name = 'pair_strands_XY'
    input: beds = expand("{d}/{P[0]}/beds/ch{P[1]}.bed",d=data_dir,P=product(cells,chambers))
    output: pairs = 'strand_pairs/chrXchrY'
    run:
        strand_pairing_analysis.pair_strands_XY(input.beds,output.pairs)
'''
# RUN HAPCUT2
rule prune_haplotype:
    params:
        job_name = "{s}.{x}.prune_haplotype",
    input:
        hapblocks = expand("{E}/hapcut2_{{s}}/{{x}}/{c}.output.uncorrected",E=experiments_dir,c=chroms)
    output:
        hapblocks = expand("{E}/hapcut2_{{s}}/{{x}}/{c}.output",E=experiments_dir,c=chroms)
    run:
        for i, o in zip(input.hapblocks,output.hapblocks):
            fileIO.prune_hapblock_file(i, o, snp_conf_cutoff=0.95, split_conf_cutoff=0, use_refhap_heuristic=True)

# RUN HAPCUT2
rule run_hapcut2:
    params:
        job_name = "{s}.{c}.{x}.hapcut",
    input:
        frag_file = "%s/{s}/fragmat/{x}/{c}" % data_dir,
        vcf_file  = lambda wildcards: expand("{v}/{c}.vcf", v=variant_vcf_dir,c=wildcards.c)
    output:
        hapblocks = "{E}/hapcut2_{s}/{x}/{c}.output.uncorrected",
        runtime = "{E}/hapcut2_{s}/{x}/{c}.runtime"
    run:
        # run hapcut
        runtime = run_tools.run_hapcut2(config['hapcut2'], input.frag_file, input.vcf_file, output.hapblocks, 5, 0.8, '--ea 1')
        with open(output.runtime,'w') as rf:
            print(runtime, file=rf)
'''
# CREATE BED FILES FOR THE NEW FIXED FRAGMENTS
rule make_fixed_beds:
    params:
        job_name  = "make_fixed_beds",
    input:
        expand("{d}/PGP1_ALL/fragmat/cov1_strict/{c}",d=data_dir,c=chroms)
    output:
        expand("{d}/{s}/fixed_beds/ch{ch}.bed",d=data_dir,s=cells,ch=chambers)
    run:
        cell_boundaries = defaultdict(list) # key: (cell,chamber#,chrom)  value: (start, end)

        for chrom in chroms:
            infile = "{}/PGP1_ALL/fragmat/cov1_strict/{}".format(data_dir,chrom)
            with open(infile,'r') as inf:
                for line in inf:
                    if len(line) < 3:
                        continue
                    el = line.strip().split()
                    ID = el[1]
                    el2 = ID.split(':')
                    boundstr = el2[-1]
                    (start, end) = [int(x) for x in boundstr.split('-')]
                    cell = el2[2]
                    chamber = el2[3]
                    chambernum = int(chamber[2:])
                    cell_boundaries[(cell,chambernum,chrom)].append((start, end))

        for cell in cells:
            for chamber in chambers:
                outfile = '{}/{}/fixed_beds/ch{}.bed'.format(data_dir,cell,chamber)
                with open(outfile,'w') as of:
                    for chrom in chroms:
                        cell_boundaries[(cell,chamber,chrom)].sort()
                        for start, end in cell_boundaries[(cell,chamber,chrom)]:
                            line = '{}\t{}\t{}\t{}\tchamber{}'.format(chrom, start, end, end-start, chamber)
                            print(line,file=of)

# COMBINE FRAGMENT MATRICES
rule fix_fragmat:
    params:
        job_name  = "fix_fragmat.{x}",
    input:
        var_vcfs = expand("{v}/{CHR}.vcf",v=variant_vcf_dir,CHR=chroms),
        P_ALL = expand("{dat}/PGP1_ALL/augmented_fragmat/{c}",dat=data_dir,c=chroms)
    output:
        fixed = expand("{{data_dir}}/PGP1_ALL/fragmat/{{x}}/{c}",c=chroms)
    run:
        mincov = 0
        if 'cov2' in wildcards.x:
            mincov = 2
        elif 'cov3' in wildcards.x:
            mincov = 3

        if 'none' in wildcards.x:
            mode = 'none'
        elif 'basic' in wildcards.x:
            mode = 'basic'
        elif 'strict' in wildcards.x:
            mode = 'strict'

        for i,v,o in zip(input.P_ALL, input.var_vcfs, output.fixed):
            fix_chamber_contamination(i,v,o,threshold=2, min_coverage=mincov,mode=mode)

# COMBINE FRAGMENT MATRICES
rule merge_fragmat:
    params:
        job_name  = "merge_fragmat",
    input:
        P21      = expand("{dat}/PGP1_21/augmented_fragmat/{c}",dat=data_dir,c=chroms),
        P22      = expand("{dat}/PGP1_22/augmented_fragmat/{c}",dat=data_dir,c=chroms),
        PA1      = expand("{dat}/PGP1_A1/augmented_fragmat/{c}",dat=data_dir,c=chroms),
        var_vcfs = expand("{v}/{CHR}.vcf",v=variant_vcf_dir,CHR=chroms)
    output:
        P_ALL = expand("{dat}/PGP1_ALL/augmented_fragmat/{c}",dat=data_dir,c=chroms)
    run:
        for i1, i2, i3, o in zip(input.P21, input.P22, input.PA1, output.P_ALL):
            shell('cat {i1} {i2} {i3} > {o}')

# CREATE AUGMENTED FRAGMENT MATRIX FILES WITH HETEROZYGOUS CALL LOCATIONS
# SAME FORMAT AS BEFORE, BUT NOW '2' represents a heterozygous call
rule create_augmented_fragmat:
    params:
        job_name  = "{s}.create_augmented_fragmat",
    input:
        ploidy1_vcfs = expand("{{data_dir}}/{{s}}/ploidy1/ch{ch}.vcf",ch=chambers),
        ploidy2_vcfs = expand("{{data_dir}}/{{s}}/ploidy2/ch{ch}.vcf",ch=chambers),
        beds         = expand("{{data_dir}}/{{s}}/beds/ch{ch}.bed",ch=chambers),
        var_vcfs     = expand("{v}/{CHR}.vcf",v=variant_vcf_dir,CHR=chroms)
    output:
        expand("{{data_dir}}/{{s}}/augmented_fragmat/{c}",c=chroms)
    run:
        output_dir = os.path.join(data_dir,wildcards.s,'augmented_fragmat')
        create_hapcut_fragment_matrices_freebayes(input.ploidy1_vcfs, input.ploidy2_vcfs, input.beds, wildcards.s, chambers_pad, input.var_vcfs, output_dir, hets_in_seq=True)
'''
# simlink data to make path naming scheme consistent between PGP1_21 and PGP1_22
rule simlinks:
    input:
        expand("{DIR}/PGP1_21_ch{chpad}.noseg.freebayes.vcf",DIR=config['PGP1_21_ploidy1'],chpad=chambers_pad),
        expand("{DIR}/PGP1_21_ch{chpad}.ploidy2.freebayes.depth.vcf",DIR=config['PGP1_21_ploidy2'],chpad=chambers_pad),
        expand("{DIR}/ch{ch}.bed",DIR=config['PGP1_21_beds'],ch=chambers),
        expand("{DIR}/PGP1_22_ch{chpad}.noseg.sorted.freebayes.vcf",DIR=config['PGP1_22_ploidy1'],chpad=chambers_pad),
        expand("{DIR}/PGP1_22_ch{chpad}.ploidy2.freebayes.vcf",DIR=config['PGP1_22_ploidy2'],chpad=chambers_pad),
        expand("{DIR}/ch{ch}.bed",DIR=config['PGP1_22_beds'],ch=chambers),
        expand("{DIR}/PGP1_A1_ch{chpad}.noseg.sorted.freebayes.vcf",DIR=config['PGP1_A1_ploidy1'],chpad=chambers_pad),
        expand("{DIR}/PGP1_A1_ch{chpad}.freebayes.ploidy2.allpos.vcf",DIR=config['PGP1_A1_ploidy2'],chpad=chambers_pad),
        expand("{DIR}/ch{ch}.bed",DIR=config['PGP1_A1_beds'],ch=chambers),
    run:
        for ch,chpad in zip(chambers,chambers_pad):
            shell('''
            mkdir -p {data_dir}/PGP1_21/ploidy1
            mkdir -p {data_dir}/PGP1_21/ploidy2
            mkdir -p {data_dir}/PGP1_21/beds
            ln -s {config[PGP1_21_ploidy1]}/PGP1_21_ch{chpad}.noseg.freebayes.vcf {data_dir}/PGP1_21/ploidy1/ch{ch}.vcf
            ln -s {config[PGP1_21_ploidy2]}/PGP1_21_ch{chpad}.ploidy2.freebayes.depth.vcf {data_dir}/PGP1_21/ploidy2/ch{ch}.vcf
            cp {config[PGP1_21_beds]}/ch{ch}.bed {data_dir}/PGP1_21/beds/ch{ch}.bed

            mkdir -p {data_dir}/PGP1_22/ploidy1
            mkdir -p {data_dir}/PGP1_22/ploidy2
            mkdir -p {data_dir}/PGP1_22/beds
            ln -s {config[PGP1_22_ploidy1]}/PGP1_22_ch{chpad}.noseg.sorted.freebayes.vcf {data_dir}/PGP1_22/ploidy1/ch{ch}.vcf
            ln -s {config[PGP1_22_ploidy2]}/PGP1_22_ch{chpad}.ploidy2.freebayes.vcf {data_dir}/PGP1_22/ploidy2/ch{ch}.vcf
            cp {config[PGP1_22_beds]}/ch{ch}.bed {data_dir}/PGP1_22/beds/ch{ch}.bed

            mkdir -p {data_dir}/PGP1_A1/ploidy1
            mkdir -p {data_dir}/PGP1_A1/ploidy2
            mkdir -p {data_dir}/PGP1_A1/beds
            ln -s {config[PGP1_A1_ploidy1]}/PGP1_A1_ch{chpad}.noseg.sorted.freebayes.vcf {data_dir}/PGP1_A1/ploidy1/ch{ch}.vcf
            ln -s {config[PGP1_A1_ploidy2]}/PGP1_A1_ch{chpad}.freebayes.ploidy2.allpos.vcf {data_dir}/PGP1_A1/ploidy2/ch{ch}.vcf
            cp {config[PGP1_A1_beds]}/ch{ch}.bed {data_dir}/PGP1_A1/beds/ch{ch}.bed
            ''')

rule clean:
    shell:
        '''
        rm -rf sissor_project/plots sissor_project/error_rates sissor_project/experiments sissor_project/data sissor_project/stdout/*
        '''
'''
rm -rf sissor_project/plots/bak sissor_project/error_rates/bak sissor_project/experiments/bak sissor_project/data/bak || true 2>/dev/null
mkdir -p sissor_project/plots/bak sissor_project/error_rates/bak sissor_project/experiments/bak sissor_project/data/bak || true 2>/dev/null
mv sissor_project/plots/*.png sissor_project/plots/bak || true 2>/dev/null
mv sissor_project/error_rates/*.p sissor_project/error_rates/bak || true 2>/dev/null
mv sissor_project/experiments/hapcut2* sissor_project/experiments/bak || true 2>/dev/null
mv sissor_project/data/PGP1_21 sissor_project/data/bak || true 2>/dev/null
mv sissor_project/data/PGP1_22 sissor_project/data/bak || true 2>/dev/null
mv sissor_project/data/PGP1_A1 sissor_project/data/bak || true 2>/dev/null
mv sissor_project/data/PGP1_21_22 sissor_project/data/bak || true 2>/dev/null
mv sissor_project/data/PGP1_ALL sissor_project/data/bak || true 2>/dev/null
'''

rule dummy:
    run:
        pass
